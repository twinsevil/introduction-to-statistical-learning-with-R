---
title: "ISLR chapter 3"
author: "Литвинцев И.С"
date: '18 августа 2018 г '
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Instal Packages 

```{r}
library(MASS)
library(ISLR)
```

## Simple linear regression 

The MASS library contains the Boston data set, which records medv (median house value) for 506 neighborhoods around Boston. We will seek to predict medv using 13 predictors such as 

- rm (average number of rooms per house),

- age (average age of houses), 

- lstat (percent of households with low
socioeconomic status).


```{r}
#?Boston
#fix(Boston)
head(Boston)

names(Boston)
```


```{r}
attach(Boston)
lm.fit <- lm(medv ~ lstat)
lm.fit
summary(lm.fit)

```


We can use the names() function in order to find out what other pieces of information are stored in lm.fit. Although we can extract these quantities by name—e.g. lm.fit$coefficients—it is safer to use the extractor functions like coef() to access them.

In order to obtain a confidence interval for the coefficient estimates, we can use the confint() command.

```{r}
names(lm.fit)
coef(lm.fit)
confint(lm.fit)
```

The predict() function can be used to produce confidence intervals and prediction intervals for the prediction of medv for a given value of lstat.

```{r}
predict(lm.fit, data.frame(lstat=c(5,10,15)), interval="confidence")
predict(lm.fit, data.frame(lstat=c(5,10,15)), interval="prediction")
```

For instance, the 95% confidence interval associated with a lstat value of 10 is (24.47, 25.63), and the 95% prediction interval is (12.828, 37.28). As expected, the confidence and prediction intervals are centered around the same point (a predicted value of 25.05 for medv when lstat equals 10), but the latter are substantially wider.

We will now plot medv and lstat along with the least squares regression
line using the plot() and abline() functions.

```{r}
plot(lstat, medv, pch =20)+
  abline(lm.fit, col="red", lwd =3)

plot(1:20 ,1:20, pch =1:20)
```

The abline() function can be used to draw any line, not just the least squares regression line. To draw a line with intercept a and slope b, we type abline(a,b).

However, it is often convenient to view all four plots together. We can achieve this by using the par() function, which tells R to split the display screen into separate panels.
For example, par(mfrow=c(2,2)) divides the plotting region into a 2 ?? 2 grid of panels.

```{r}
par(mfrow =c(2,2))
plot(lm.fit)
```

Alternatively, we can compute the residuals from a linear regression fit using the residuals() function. The function rstudent() will return the 
studentized residuals, and we can use this function to plot the residuals against the fitted values.

```{r}
plot(predict(lm.fit), residuals(lm.fit))
plot(predict(lm.fit), rstudent(lm.fit))
plot(predict(lm.fit), sqrt(abs(rstudent(lm.fit)))) # то же что ‘Scale-Location’ plot
plot(hatvalues(lm.fit), sqrt(abs(rstudent(lm.fit)))) # то же что Residual-Leverage plot
```

On the basis of the residual plots, there is some evidence of non-linearity. Leverage statistics can be computed for any number of predictors using the hatvalues() function.

```{r}
plot(hatvalues(lm.fit))
which.max(hatvalues(lm.fit))
```

The which.max() function identifies the index of the largest element of a vector. In this case, it tells us which observation has the largest leverage statistic.

## Multiple Linear Regression

```{r}
lm.fit = lm(medv~lstat+age, data=Boston)
summary(lm.fit)
```

The Boston data set contains 13 variables, and so it would be cumbersome (громоздко) to have to type all of these in order to perform a regression using all of the predictors. Instead, we can use the following short-hand:

```{r}
lm.fit <- lm(medv ~ ., Boston)
summary(lm.fit)
```

We can access the individual components of a summary object by name (type `?summary.lm` to see what is available). Hence `summary(lm.fit)\$r.sq` gives us the R2, and `summary(lm.fit)\$sigma` gives us the RSE. The `vif()` function, part of the car package, can be used to compute variance inflation factors. Most VIF’s are low to moderate for this data (Для этих данных большинство VIF являются низкими или средними). The car package is not part of the base R installation so it must be downloaded the first time you use it via the `install.packages` option in R.

```{r}
library(car)
vif(lm.fit)
```

What if we would like to perform a regression using all of the variables but one? For example, in the above regression output, age has a high p-value. So we may wish to run a regression excluding this predictor. The following syntax results in a regression using all predictors except age.

```{r}
lm.fit1 <- lm(medv ~ .-age, Boston)
summary(lm.fit1)
```

Alternatively, the `update()` function can be used.

```{r}
lm.fit1 <- update(lm.fit1, ~ .-age)
```

## Interaction Terms

It is easy to include interaction terms in a linear model using the `lm()` function. The syntax `lstat:black` tells R to include an interaction term between lstat and black. The syntax `lstat*age` simultaneously includes lstat, age, and the interaction term lstat??age as predictors; it is a shorthand for `lstat+age+lstat:age`.

```{r}
summary(lm(medv ~ lstat*age, data=Boston))
summary(lm(medv ~ lstat:age, data=Boston))
```

## Non-linear Transformations of the Predictors

The `lm()` function can also accommodate (применять) non-linear transformations of the predictors. For instance, given a predictor X, we can create a predictor X2 using `I(X^2)`. The function `I()` is needed since the `^` has a special meaning in a formula; wrapping as we do allows the standard usage in R, which is to raise X to the power 2.(Такая обертка позволяет возвести X в степень согласно стандартному использованию R)
We now perform a regression of medv onto `lstat` and `lstat^2`.

```{r}
lm.fit2 <- lm(medv ~ lstat+I(lstat^2), Boston)
summary(lm.fit2)
```


The near-zero p-value associated with the quadratic term suggests that it leads to an improved model. We use the `anova()` function to further quantify the extent to which the quadratic fit is superior to the linear fit. (Мы используем функцию `anova ()` для дальнейшего количественного определения степени, в которой квадратичная подгонка превосходит линейную)

```{r}
lm.fit <- lm(medv ~ lstat)
anova(lm.fit, lm.fit2)

```

Here Model 1 represents the linear submodel containing only one predictor, `lstat`, while Model 2 corresponds to the larger quadratic model that has two predictors, `lstat` and `lstat2`. The `anova()` function performs a hypothesis test comparing the two models. The null hypothesis is that the two models fit the data equally well, and the alternative hypothesis is that the full model is superior (полная модель лучше). Here the F-statistic is 135 and the associated p-value is virtually zero. This provides very clear evidence that the model containing the predictors `lstat` and `lstat2` is far superior to the model that only contains the predictor `lstat`. This is not surprising, since earlier we saw evidence for non-linearity in the relationship between `medv` and `lstat`. If we type

```{r}
par(mfrow=c(2,2))
plot(lm.fit2)
```

then we see that when the lstat2 term is included in the model, there is little discernible (слабо различимый) pattern in the residuals.


In order to create a cubic fit, we can include a predictor of the form `I(X^3)`. However, this approach can start to get cumbersome (громоздкий) for higherorder polynomials. A better approach involves using the poly() function to create the polynomial within lm(). For example, the following command produces a fifth-order polynomial fit:

```{r}
lm.fit5 <- lm(medv ~ poly(lstat, 5))
summary(lm.fit5)


plot(lstat, medv)+
  lines(sort(lstat), fitted(lm.fit5)[order(lstat)], col="red", lwd = 3) 


```

This suggests that including additional polynomial terms, up to fifth order, leads to an improvement in the model fit! However, further investigation of the data reveals that no polynomial terms beyond fifth order have significant p-values in a regression fit (никакие полиномиальные члены за пятым порядком не имеют значительных p-значений в регрессионной подгонке).

Of course, we are in no way restricted to using polynomial transformations of the predictors (Конечно, мы никоим образом не ограничены использованием полиномиальных преобразований предикторов). Here we try a log transformation.

```{r}
summary(lm(medv ~ log(rm), data=Boston))
```

## Qualitative Predictors

We will attempt to predict Sales (child car seat sales) in 400 locations based on a number of predictors.

```{r}
fix(Carseats)
head(Carseats)
names(Carseats)
```

The Carseats data includes qualitative predictors such as `Shelveloc`, an indicator of the quality of the shelving location—that is, the space within a store in which the car seat is displayed—at each location.

Given a qualitative variable such as Shelveloc, R generates dummy variables automatically. Below we fit a multiple regression model that includes some interaction terms.

```{r}
lm.fit <- lm(Sales ~ . + Income:Advertising + Price:Age, data=Carseats)
summary(lm.fit)
```

The `contrasts()` function returns the coding that R uses for the dummy variables. Use `?contrasts` to learn about other contrasts, and how to set them.

```{r}
attach(Carseats)
contrasts(ShelveLoc)
```

R has created a `ShelveLocGood` dummy variable that takes on a value of 1 if the shelving location is good, and 0 otherwise. It has also created a `ShelveLocMedium` dummy variable that equals 1 if the shelving location is medium, and 0 otherwise. A bad shelving location corresponds to a zero for each of the two dummy variables. The fact that the coefficient for `ShelveLocGood` in the regression output is positive indicates that a good shelving location is associated with high sales (relative to a bad location). And `ShelveLocMedium` has a smaller positive coefficient, indicating that a medium shelving location leads to higher sales than a bad shelving location but lower sales than a good shelving location.

In case of simple linear regression with qualitative variables `ShelveLoc`, we have 
$$X_1 = \begin{cases} 1, & \mbox{if shelving location is good }  \\ 0, & \mbox{else } \\ 0,   & \mbox{else } \end{cases} $$
$$X_2 = \begin{cases} 0, & \mbox{else }  \\ 1, & \mbox{if shelving location is medium } \\ 0,   & \mbox{else } \end{cases} $$

$$ y = \beta_0 + \beta_1 X_1 +\beta_2 X_2 + \varepsilon = \begin{cases} \beta_0 + \beta_1 + \varepsilon, & \mbox{if shelving location is good}  \\ \beta_0 + \beta_2 + \varepsilon, & \mbox{if shelving location is medium}  \\ \beta_0 + \varepsilon,   & \mbox{if shelving location is bad } \end{cases}$$

Сделаем линейную регрессию на Price и ShelveLoc. 

```{r}
lm.fit <- lm(Sales ~ ShelveLoc + Price)
summary(lm.fit)

plot(lm.fit)
```

Нарисуем полученные линии регрессии на фоне данных.

```{r}
Bad <- ssp[ , "ShelveLoc"]=="Bad"
Good <- ssp[ , "ShelveLoc"]=="Good"
Medium <- ssp[ , "ShelveLoc"]=="Medium"

{plot(Price[Bad], fitted(lm.fit)[Bad], type="l", col=1, ylim=c(1,20), xlim=c(20,190), xlab="Price", ylab="Fitted Sales")+
  lines(Price[Good], fitted(lm.fit)[Good], col=2)+
  lines(Price[Medium], fitted(lm.fit)[Medium], col=3)
  legend("topright", legend = c("Bad", "Good", "Medium"), col=c(1,2,3), lty=1, lwd=2)
  
  points(Price[Bad], Sales[Bad])
  points(Price[Medium], Sales[Medium], col=3)
  points(Price[Good], Sales[Good], col=2)
}
```


https://statlearning.wordpress.com/2014/01/05/chapter-3-linear-regression-applied-exercises/
http://www.timahn.com/512/TAhn_HW_3.html


